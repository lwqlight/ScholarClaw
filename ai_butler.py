import os
import requests
import yaml
import json
from zhipuai import ZhipuAI
import schedule
import time
from datetime import datetime
from dotenv import load_dotenv

# ================= 1. ç¯å¢ƒä¸é…ç½®åˆå§‹åŒ– =================
load_dotenv()
ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")

if not ZHIPU_API_KEY or not FEISHU_WEBHOOK_URL:
    print("âŒ è‡´å‘½é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° API Key æˆ– Webhook é“¾æ¥ï¼è¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
    exit(1)

try:
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
        TARGET_KEYWORDS = config.get("keywords", [])
        TARGET_VENUES = config.get("venues", "")
        SCHEDULE_TIMES = config.get("schedule_times", ["08:30", "18:30"])
        
        # ğŸ’¡ æ–°çš„é…ç½®è¯»å–é€»è¾‘
        MAX_PER_KEYWORD = config.get("max_papers_per_keyword", 1)
        MAX_TOTAL_PUSH = config.get("max_total_push", 5)
except FileNotFoundError:
    print("âŒ è‡´å‘½é”™è¯¯ï¼šæ‰¾ä¸åˆ° config.yaml é…ç½®æ–‡ä»¶ï¼")
    exit(1)

client = ZhipuAI(api_key=ZHIPU_API_KEY)

# ================= 2. å†å²è®°å¿†è¯»å– (æ°¸ä¹…å»é‡) =================
HISTORY_FILE = "history.json"

def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_history(history_list):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history_list, f, ensure_ascii=False, indent=2)

# ================= 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
def fetch_top_tier_papers():
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ“¡ å…·èº«é›·è¾¾å¯åŠ¨ï¼æ­£åœ¨æ‰«æå…¨çƒæœºå™¨äººé¡¶ä¼š/é¡¶åˆŠ...")
    unique_papers = {} 
    current_year = datetime.now().year
    year_range = f"{current_year-1}-{current_year}" 
    
    pushed_history = load_history()

    for keyword in TARGET_KEYWORDS:
        print(f"  -> æ­£åœ¨æ£€ç´¢å…³é”®è¯: {keyword} ...")
        url = "https://api.semanticscholar.org/graph/v1/paper/search"
        params = {
            "query": keyword,
            "venue": TARGET_VENUES,
            "year": year_range,
            "fields": "title,abstract,url,venue,year,authors,publicationDate",
            "limit": 10 
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data:
                    added_for_this_keyword = 0  # ğŸ’¡ è®°å½•å½“å‰å…³é”®è¯æ‰¾åˆ°äº†å‡ ç¯‡æ–°è®ºæ–‡
                    
                    for paper in data['data']:
                        if not paper.get('abstract'): 
                            continue
                            
                        raw_title = paper.get('title')
                        
                        # å¦‚æœè¿™ç¯‡è®ºæ–‡ä¹‹å‰æ²¡è¢«æ¨è¿‡
                        if raw_title not in unique_papers and raw_title not in pushed_history:
                            # ğŸ’¡ æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœè¿™ä¸ªé¢†åŸŸå·²ç»æŠ“å¤Ÿäº†åé¢ï¼Œå°±è·³å‡ºå¾ªç¯ï¼ŒæŠŠæœºä¼šç•™ç»™ä¸‹ä¸€ä¸ªé¢†åŸŸï¼
                            if added_for_this_keyword >= MAX_PER_KEYWORD:
                                break 
                                
                            venue_name = paper.get('venue', 'é¡¶çº§ä¼šè®®')
                            pub_date = paper.get('publicationDate') or str(paper.get('year', current_year))
                            
                            authors_list = paper.get('authors', [])
                            author_names = [a.get('name') for a in authors_list if a.get('name')]
                            if len(author_names) > 3:
                                author_str = ", ".join(author_names[:3]) + " ç­‰"
                            else:
                                author_str = ", ".join(author_names) if author_names else "æœªçŸ¥"
                            
                            unique_papers[raw_title] = {
                                "title": raw_title, 
                                "venue": venue_name,
                                "date": pub_date,
                                "authors": author_str,
                                "link": paper.get('url', 'https://www.semanticscholar.org/'),
                                "summary": paper.get('abstract'),
                                "raw_title": raw_title 
                            }
                            added_for_this_keyword += 1
                            
            time.sleep(1) 
        except Exception as e:
            print(f"æ£€ç´¢ {keyword} æ—¶ç½‘ç»œå¼€å°å·®äº†: {e}")
            
    # ğŸ’¡ æœ€ç»ˆé™åˆ¶æ€»æ•°ï¼Œé˜²æ­¢ä¸€æ¬¡æ€§è½°ç‚¸é£ä¹¦
    return list(unique_papers.values())[:MAX_TOTAL_PUSH] 

def ai_summarize(paper):
    print(f"æ­£åœ¨è¯·æ™ºè°±AIç²¾è¯»é¡¶ä¼šæ–‡ç« : {paper['title'][:30]}...")
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„å…·èº«æ™ºèƒ½ä¸æœºå™¨äººå­¦æœ¯åŠ©ç†ã€‚è¯·é˜…è¯»ä»¥ä¸‹è®ºæ–‡æ‘˜è¦ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æˆ‘æä¾›çš„ Markdown æ ¼å¼è¾“å‡ºæ€»ç»“ã€‚
    
    âš ï¸ æ³¨æ„ï¼šç¦æ­¢è¾“å‡ºä»»ä½•â€œå¥½çš„â€ã€â€œè¿™ç¯‡è®ºæ–‡â€ç­‰å‰ç¼€åºŸè¯ï¼Œä¸¥æ ¼ä¿æŒå®¢è§‚å†·é…·çš„å­¦æœ¯è¯­è°ƒï¼Œç›´æ¥è¾“å‡ºä»¥ä¸‹ç»“æ„ï¼š
    
    **ğŸ¯ æ ¸å¿ƒç—›ç‚¹ï¼š**
    (ç”¨ä¸€å¥è¯æå…¶ç²¾ç‚¼åœ°æŒ‡å‡ºä¼ ç»Ÿæ–¹æ³•æˆ–å½“å‰è¡Œä¸šçš„å±€é™æ€§)
    
    **ğŸ› ï¸ æŠ€æœ¯è·¯çº¿ï¼š**
    (ç”¨1-2å¥è¯æ¦‚æ‹¬ä½œè€…ä½¿ç”¨äº†ä»€ä¹ˆæ ¸å¿ƒç®—æ³•ã€æ¶æ„ã€æ•°æ®é›†æˆ–ç‰©ç†è®¾è®¡æ¥è§£å†³ä¸Šè¿°é—®é¢˜)
    
    **âœ¨ åˆ›æ–°çªç ´ï¼š**
    â€¢ (æ ¸å¿ƒåˆ›æ–°ç‚¹æˆ–å®éªŒè·‘åˆ†ç»“æœ1ï¼Œä¸è¶…è¿‡30ä¸ªå­—)
    â€¢ (æ ¸å¿ƒåˆ›æ–°ç‚¹æˆ–å®éªŒè·‘åˆ†ç»“æœ2ï¼Œä¸è¶…è¿‡30ä¸ªå­—)
    
    ---
    è®ºæ–‡æ ‡é¢˜ï¼š{paper['title']}
    è®ºæ–‡æ‘˜è¦ï¼š{paper['summary']}
    """
    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå†·é…·ã€ç²¾ç‚¼çš„é¡¶çº§å­¦æœ¯æœºå™¨ï¼Œç»å¯¹éµå¾ªè¾“å‡ºæ ¼å¼è§„èŒƒã€‚"},
                {"role": "user", "content": prompt}
            ],
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"**âš ï¸ AIæ€»ç»“å¤±è´¥ï¼š** {e}"

def push_to_feishu(paper):
    print("æ­£åœ¨æ¨é€ç¡¬æ ¸æƒ…æŠ¥åˆ°é£ä¹¦...")
    venue = paper.get('venue', 'é¡¶çº§ä¼šè®®')
    title = paper['title']
    authors = paper['authors']
    date = paper['date']
    ai_summary = paper['ai_summary']
    link = paper['link']

    payload = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": f"ğŸ‘‘ {venue} æœ€æ–°æ”¶å½• (ç®¡å®¶ç‰¹ä¾›)"},
                "template": "red" 
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": f"**ã€Š{title}ã€‹**\n\n<font color='grey'>ğŸ‘¥ ä½œè€…ï¼š{authors}</font>\n<font color='grey'>ğŸ“… å‘è¡¨æ—¥æœŸï¼š{date}</font>\n\n---\n\n{ai_summary}"
                },
                {"tag": "hr"},
                {
                    "tag": "action",
                    "actions": [
                        {
                            "tag": "button",
                            "text": {"tag": "plain_text", "content": "ğŸ”— ç‚¹å‡»é˜…è¯»åŸæ–‡"},
                            "type": "primary",
                            "url": link
                        }
                    ]
                }
            ]
        }
    }
    response = requests.post(FEISHU_WEBHOOK_URL, json=payload)
    if response.json().get("code") == 0:
        print("âœ… é£ä¹¦æƒ…æŠ¥æ¨é€æˆåŠŸï¼")
    else:
        print(f"âŒ é£ä¹¦æ¨é€å¤±è´¥è¢«æ‹¦æˆªï¼æŠ¥é”™ä¿¡æ¯: {response.json()}")

def push_empty_notice_to_feishu():
    print("æ²¡æœ‰å‘ç°æ–°è®ºæ–‡ï¼Œæ­£åœ¨å‘é£ä¹¦æ±‡æŠ¥å¹³å®‰...")
    payload = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": "â˜• é¡¶ä¼šé›·è¾¾æ‰«æå®Œæ¯• (ç®¡å®¶æŠ¥å¤‡)"},
                "template": "grey"
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": "**æŠ¥å‘Šè€æ¿ï¼š**\n\nåˆšåˆšå®Œæˆäº†ä¸€æ¬¡å…¨çƒæœºå™¨äººé¡¶ä¼š/é¡¶åˆŠçš„æ·±åº¦æ‰«æã€‚\n\n**ğŸ” ç»“æœï¼š** è¿‡å»å‡ ä¸ªå°æ—¶å†…ï¼Œåœ¨æ‚¨çš„**æ‰€æœ‰å…³æ³¨é¢†åŸŸ**å‡æœªå‘ç°æœªè¯»çš„é«˜ä»·å€¼æ–°è®ºæ–‡ã€‚\n\næ‚¨å¯ä»¥å®‰å¿ƒå–æ¯å’–å•¡ï¼ŒEmboRadar ä¼šç»§ç»­åœ¨åå°ä¸ºæ‚¨ç›¯ç›˜ï¼â˜•ï¸"
                }
            ]
        }
    }
    requests.post(FEISHU_WEBHOOK_URL, json=payload)
    print("âœ… æ— æ›´æ–°å¹³å®‰æŠ¥å¤‡æ¨é€æˆåŠŸï¼")

def job():
    papers = fetch_top_tier_papers()
    
    # ğŸ’¡ åªæœ‰å½“æ‰€æœ‰é¢†åŸŸåŠ èµ·æ¥éƒ½æ²¡æœ‰1ç¯‡æ–°è®ºæ–‡æ—¶ï¼Œæ‰ä¼šè§¦å‘å¹³å®‰é€»è¾‘
    if not papers:
        push_empty_notice_to_feishu()
        return
        
    pushed_history = load_history()
    
    for paper in papers:
        paper['ai_summary'] = ai_summarize(paper)
        push_to_feishu(paper)
        
        pushed_history.append(paper["raw_title"])
        save_history(pushed_history)
        
        time.sleep(2)
        
    print("é¡¶ä¼šæƒ…æŠ¥æ±‡æŠ¥å®Œæ¯•ï¼")

# ================= 4. ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    print("å¯åŠ¨æˆåŠŸï¼EmboRadar (å…·èº«é›·è¾¾) å·²åœ¨åå°å¾…å‘½...")
    
    for t in SCHEDULE_TIMES:
        schedule.every().day.at(t).do(job)
        print(f"å·²è®¾å®šå®šæ—¶ä»»åŠ¡: æ¯å¤© {t} è‡ªåŠ¨æ‰«æ")

    print("æ­£åœ¨è¿›è¡Œé¦–æ¬¡é›·è¾¾æ‰«æï¼Œè¯·ç¨å€™...")
    job() 

    while True:
        schedule.run_pending()
        time.sleep(60)