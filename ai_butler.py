import os
import requests
import yaml
import json
from zhipuai import ZhipuAI
import schedule
import time
from datetime import datetime
from dotenv import load_dotenv

# ================= 1. ç¯å¢ƒä¸é…ç½®åˆå§‹åŒ– =================
load_dotenv()
ZHIPU_API_KEY = os.getenv("ZHIPU_API_KEY")
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")

if not ZHIPU_API_KEY or not FEISHU_WEBHOOK_URL:
    print("âŒ è‡´å‘½é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° API Key æˆ– Webhook é“¾æ¥ï¼")
    exit(1)

try:
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
        TARGET_KEYWORDS = config.get("keywords", [])
        TARGET_VENUES = config.get("venues", "")
        SCHEDULE_TIMES = config.get("schedule_times", ["08:30", "18:30"])
        # ğŸ’¡ è¯»å–é…ç½®æ–‡ä»¶é‡Œçš„æ¨é€æ•°é‡ï¼Œé»˜è®¤æ˜¯ 3
        MAX_PAPERS = config.get("max_papers_per_push", 3)
except FileNotFoundError:
    print("âŒ è‡´å‘½é”™è¯¯ï¼šæ‰¾ä¸åˆ° config.yaml é…ç½®æ–‡ä»¶ï¼")
    exit(1)

client = ZhipuAI(api_key=ZHIPU_API_KEY)

# ================= 2. å†å²è®°å¿†è¯»å– (çœŸæ­£çš„æ°¸ä¹…å»é‡) =================
HISTORY_FILE = "history.json"

def load_history():
    """è¯»å–å·²ç»æ¨é€è¿‡çš„è®ºæ–‡æ ‡é¢˜è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_history(history_list):
    """ä¿å­˜æ¨é€è¿‡çš„è®ºæ–‡æ ‡é¢˜åˆ°æœ¬åœ°"""
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history_list, f, ensure_ascii=False, indent=2)

# ================= 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================
def fetch_top_tier_papers():
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] ğŸ“¡ å…·èº«é›·è¾¾å¯åŠ¨ï¼æ­£åœ¨æ‰«æå…¨çƒæœºå™¨äººé¡¶ä¼š/é¡¶åˆŠ...")
    unique_papers = {} 
    current_year = datetime.now().year
    year_range = f"{current_year-1}-{current_year}" 
    
    # åŠ è½½å°æœ¬æœ¬ï¼Œçœ‹çœ‹ä»¥å‰æ¨è¿‡ä»€ä¹ˆ
    pushed_history = load_history()

    for keyword in TARGET_KEYWORDS:
        print(f"  -> æ­£åœ¨æ£€ç´¢å…³é”®è¯: {keyword} ...")
        url = "https://api.semanticscholar.org/graph/v1/paper/search"
        params = {
            "query": keyword,
            "venue": TARGET_VENUES,
            "year": year_range,
            "fields": "title,abstract,url,venue,year",
            # è¿™é‡Œçš„ limit è®¾å¾—ç¨å¾®å¤§ä¸€ç‚¹(æ¯”å¦‚10)ï¼Œä¸ºäº†è·å–è¶³å¤Ÿå¤šçš„åŸºæ•°æ¥è¿›è¡Œå»é‡è¿‡æ»¤
            "limit": 10 
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data:
                    for paper in data['data']:
                        if not paper.get('abstract'): 
                            continue
                            
                        raw_title = paper.get('title')
                        
                        # ğŸ’¡ ç»ˆæå»é‡é€»è¾‘ï¼šä¸ä»…æœ¬æ¬¡å¾ªç¯æ²¡å‡ºç°è¿‡ï¼Œè€Œä¸”å†å²å°æœ¬æœ¬é‡Œä¹Ÿæ²¡å‡ºç°è¿‡ï¼
                        if raw_title not in unique_papers and raw_title not in pushed_history:
                            venue_name = paper.get('venue', 'é¡¶çº§ä¼šè®®')
                            year = paper.get('year', current_year)
                            unique_papers[raw_title] = {
                                "title": f"[{venue_name} {year}] {raw_title}", 
                                "link": paper.get('url', 'https://www.semanticscholar.org/'),
                                "summary": paper.get('abstract'),
                                "raw_title": raw_title # ä¿å­˜åŸå§‹æ ‡é¢˜ç”¨äºè®°å½•å†å²
                            }
            time.sleep(1) 
        except Exception as e:
            print(f"æ£€ç´¢ {keyword} æ—¶ç½‘ç»œå¼€å°å·®äº†: {e}")
            
    # ğŸ’¡ ä½¿ç”¨é…ç½®é¡¹ MAX_PAPERS æ§åˆ¶æœ€ç»ˆè¿”å›çš„æ•°é‡
    return list(unique_papers.values())[:MAX_PAPERS] 

def ai_summarize(paper):
    print(f"æ­£åœ¨è¯·æ™ºè°±AIç²¾è¯»é¡¶ä¼šæ–‡ç« : {paper['title'][:30]}...")
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„å…·èº«æ™ºèƒ½ä¸æœºå™¨äººå­¦æœ¯åŠ©ç†ã€‚è¯·é˜…è¯»ä»¥ä¸‹è¿™ç¯‡æœ€æ–°å‘è¡¨åœ¨é¡¶çº§ä¼šè®®/æœŸåˆŠä¸Šçš„è®ºæ–‡æ‘˜è¦ï¼Œç”¨ä¸­æ–‡ä¸ºæˆ‘æ€»ç»“ã€‚
    è¦æ±‚ï¼š
    1. ç”¨ä¸€å¥å¤§ç™½è¯æ¦‚æ‹¬å®ƒè§£å†³äº†ä»€ä¹ˆè¡Œä¸šç—›ç‚¹ã€‚
    2. åˆ—å‡º2-3ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹ã€‚
    3. è¯­æ°”ä¸“ä¸šä¸”ç²¾ç‚¼ã€‚
    
    è®ºæ–‡æ ‡é¢˜ï¼š{paper['title']}
    è®ºæ–‡æ‘˜è¦ï¼š{paper['summary']}
    """
    try:
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å­¦æœ¯åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AIæ€»ç»“å¤±è´¥: {e}"

def push_to_feishu(paper_title, ai_summary, paper_link):
    print("æ­£åœ¨æ¨é€ç¡¬æ ¸æƒ…æŠ¥åˆ°é£ä¹¦...")
    payload = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": "ğŸ‘‘ é¡¶ä¼šæƒ…æŠ¥é€Ÿé€’ (ç®¡å®¶ç‰¹ä¾›)"},
                "template": "red" 
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": f"**ğŸ“„ è®ºæ–‡æ ‡é¢˜ï¼š**\n{paper_title}\n\n**ğŸ’¡ æ ¸å¿ƒæç‚¼ï¼š**\n{ai_summary}"
                },
                {"tag": "hr"},
                {
                    "tag": "action",
                    "actions": [
                        {
                            "tag": "button",
                            "text": {"tag": "plain_text", "content": "ğŸ”— ç‚¹å‡»é˜…è¯»åŸæ–‡"},
                            "type": "primary",
                            "url": paper_link
                        }
                    ]
                }
            ]
        }
    }
    response = requests.post(FEISHU_WEBHOOK_URL, json=payload)
    if response.json().get("code") == 0:
        print("âœ… é£ä¹¦æƒ…æŠ¥æ¨é€æˆåŠŸï¼")
    else:
        print(f"âŒ é£ä¹¦æ¨é€å¤±è´¥è¢«æ‹¦æˆªï¼æŠ¥é”™ä¿¡æ¯: {response.json()}")

def push_empty_notice_to_feishu():
    print("æ²¡æœ‰å‘ç°æ–°è®ºæ–‡ï¼Œæ­£åœ¨å‘é£ä¹¦æ±‡æŠ¥å¹³å®‰...")
    payload = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": "â˜• é¡¶ä¼šé›·è¾¾æ‰«æå®Œæ¯• (ç®¡å®¶æŠ¥å¤‡)"},
                "template": "grey"
            },
            "elements": [
                {
                    "tag": "markdown",
                    "content": "**æŠ¥å‘Šè€æ¿ï¼š**\n\nåˆšåˆšå®Œæˆäº†ä¸€æ¬¡å…¨çƒæœºå™¨äººé¡¶ä¼š/é¡¶åˆŠçš„æ·±åº¦æ‰«æã€‚\n\n**ğŸ” ç»“æœï¼š** è¿‡å»å‡ ä¸ªå°æ—¶å†…ï¼Œåœ¨æ‚¨çš„å…³æ³¨é¢†åŸŸ**æ²¡æœ‰**å‘ç°æœªè¯»çš„é«˜ä»·å€¼æ–°è®ºæ–‡ã€‚\n\næ‚¨å¯ä»¥å®‰å¿ƒå–æ¯å’–å•¡ï¼Œå…·èº«é›·è¾¾ä¼šç»§ç»­åœ¨åå°ä¸ºæ‚¨ç›¯ç›˜ï¼â˜•ï¸"
                }
            ]
        }
    }
    requests.post(FEISHU_WEBHOOK_URL, json=payload)
    print("âœ… æ— æ›´æ–°å¹³å®‰æŠ¥å¤‡æ¨é€æˆåŠŸï¼")

def job():
    papers = fetch_top_tier_papers()
    if not papers:
        push_empty_notice_to_feishu()
        return
        
    pushed_history = load_history()
    
    for paper in papers:
        summary = ai_summarize(paper)
        push_to_feishu(paper["title"], summary, paper["link"])
        
        # ğŸ’¡ æ¨é€æˆåŠŸåï¼ŒæŠŠè®ºæ–‡åŸå§‹æ ‡é¢˜è®°å…¥æœ¬åœ°å†å²åº“
        pushed_history.append(paper["raw_title"])
        save_history(pushed_history)
        
        time.sleep(2)
        
    print("é¡¶ä¼šæƒ…æŠ¥æ±‡æŠ¥å®Œæ¯•ï¼")

# ================= 4. ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    print("å¯åŠ¨æˆåŠŸï¼EmboRadar (å…·èº«é›·è¾¾) å·²åœ¨åå°å¾…å‘½...")
    
    for t in SCHEDULE_TIMES:
        schedule.every().day.at(t).do(job)
        print(f"å·²è®¾å®šå®šæ—¶ä»»åŠ¡: æ¯å¤© {t} è‡ªåŠ¨æ‰«æ")

    print("æ­£åœ¨è¿›è¡Œé¦–æ¬¡é›·è¾¾æ‰«æï¼Œè¯·ç¨å€™...")
    job() 

    while True:
        schedule.run_pending()
        time.sleep(60)